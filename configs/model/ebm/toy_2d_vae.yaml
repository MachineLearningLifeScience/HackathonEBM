_target_: src.models.EBMTilting
_recursive_: false  

wgan_gp_lambda: 0.0
l2_energy_lambda: 1e-2
l2_params_lambda: 0.

base_model: "./logs/EBM_Hackathon/mog_vae_biased-2025-11-13T15-36-43/checkpoints/last.ckpt"

energy_net:
  _target_: src.layers.MLP
  input_dim: 2
  hidden_dims: [32,32,32]
  output_dim: 1
  activation: "swish"
  
sampler:
  _target_: src.samplers.SGLD
  input_dim: ${data.dim}
  input_shape: ${data.shape}  
  sample_size: ${train.batch_size}
  steps: 200
  step_size: 0.08       
  noise_std: 0.25       # Even more noise for better mode coverage
  clip_grads: 1.0       
  clamp: false          
