_target_: src.models.EBMTilting
_recursive_: false  

energy_scale: 5.0
wgan_gp_lambda: 0.0
l2_energy_lambda: 1e-4
l2_params_lambda: 1e-5

base_model: "./logs/EBM_Hackathon/mog_vae_biased-2025-11-13T15-36-43/checkpoints/last.ckpt"

energy_net:
  _target_: src.layers.MLP
  input_dim: 2
  hidden_dims: [32,32]
  output_dim: 1
  activation: "swish"
  
sampler:
  _target_: src.samplers.SGLD
  input_dim: ${data.dim}
  input_shape: ${data.shape}  
  sample_size: ${train.batch_size}
  steps: 50
  step_size: 1e-2
  noise_std: 0.5
  clip_grads: true   
